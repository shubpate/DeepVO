{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from torch.autograd import Function\n",
    "from torch.autograd import Variable\n",
    "from torchvision import models\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get resized image from a provided path\n",
    "def get_image(path,img_size= (1280,384)):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.resize(img, img_size, cv2.INTER_LINEAR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to laod images from a given directory and forming batches\n",
    "def load_images(img_dir, img_size):\n",
    "    print (\"images \", img_dir)\n",
    "    images= []\n",
    "    images_set =[]\n",
    "    for img in glob.glob(img_dir+'/*'):\n",
    "        images.append(get_image(img,img_size))\n",
    "    for i in range(len(images)-1):\n",
    "        img1 = images[i]\n",
    "        img2 = images[i+1]\n",
    "        img = np.concatenate([img1, img2],axis = -1)\n",
    "        images_set.append(img)\n",
    "    print(\"images count : \",len(images_set))\n",
    "    images_set = np.reshape(images_set, (-1, 6, 384, 1280))\n",
    "    return images_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions for pose preprocessing \n",
    "def isRotationMatrix(R):\n",
    "    \"\"\" Checks if a matrix is a valid rotation matrix\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles/\n",
    "    \"\"\"\n",
    "    Rt = np.transpose(R)\n",
    "    shouldBeIdentity = np.dot(Rt, R)\n",
    "    I = np.identity(3, dtype = R.dtype)\n",
    "    n = np.linalg.norm(I - shouldBeIdentity)\n",
    "    return n < 1e-6\n",
    "\n",
    "def rotationMatrixToEulerAngles(R):\n",
    "    \"\"\" calculates rotation matrix to euler angles\n",
    "        referred from https://www.learnopencv.com/rotation-matrix-to-euler-angles\n",
    "    \"\"\"\n",
    "    assert(isRotationMatrix(R))\n",
    "    sy = math.sqrt(R[0,0] * R[0,0] +  R[1,0] * R[1,0])\n",
    "    singular = sy < 1e-6\n",
    "\n",
    "    if  not singular :\n",
    "        x = math.atan2(R[2,1] , R[2,2])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = math.atan2(R[1,0], R[0,0])\n",
    "    else :\n",
    "        x = math.atan2(-R[1,2], R[1,1])\n",
    "        y = math.atan2(-R[2,0], sy)\n",
    "        z = 0\n",
    "\n",
    "    return np.array([x, y, z])\n",
    "\n",
    "def getMatrices(all_poses):\n",
    "    all_matrices = []\n",
    "    for i in range(len(all_poses)):\n",
    "        #print(\"I: \",i)\n",
    "        j = all_poses[i]\n",
    "        #print(\"J:   \",j)\n",
    "        p = np.array([j[3], j[7], j[11]])\n",
    "        #print(\"P:   \", p)\n",
    "        R = np.array([[j[0],j[1],j[2]],\n",
    "                [j[4],j[5],j[6]],\n",
    "                [j[8],j[9],j[10]]])\n",
    "        #print(\"R:   \", R)\n",
    "        angles = rotationMatrixToEulerAngles(R)\n",
    "        #print(\"Angles: \",angles)\n",
    "        matrix = np.concatenate((p,angles))\n",
    "        #print(\"MATRIX: \", matrix)\n",
    "        all_matrices.append(matrix)\n",
    "    return all_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to get poses form a given location \n",
    "def load_poses(pose_file):\n",
    "    print (\"pose \",pose_file)\n",
    "    poses = []\n",
    "    poses_set = []\n",
    "    with open(pose_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            pose = np.fromstring(line, dtype=float, sep=' ')\n",
    "            poses.append(pose)\n",
    "    poses = getMatrices(poses)\n",
    "    for i in range(len(poses)-1):\n",
    "        pose1 = poses[i]\n",
    "        pose2 = poses[i+1]\n",
    "        finalpose = pose2-pose1\n",
    "        poses_set.append(finalpose)\n",
    "    print(\"poses count: \",len(poses_set))\n",
    "    return poses_set          \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primary dataloader function to get both images and poses\n",
    "def VODataLoader(datapath,img_size= (1280,384), test=False):\n",
    "    print (datapath)\n",
    "    poses_path = os.path.join(datapath,'data_odometry_gray\\\\dataset\\\\poses')\n",
    "    img_path = os.path.join(datapath,'data_odometry_gray\\\\dataset\\\\sequences')\n",
    "    if test:\n",
    "        sequences = ['03']  #Kindly use this sequence only for testing as this has mininum number of images\n",
    "    else:\n",
    "        #Uncomment below and comment the next to next line to work with larger data \n",
    "        #sequences= ['01','03','06']\n",
    "        sequences = ['03']  \n",
    "        \n",
    "    images_set = []\n",
    "    odometry_set = []\n",
    "    \n",
    "    for sequence in sequences:\n",
    "        images_set.append(torch.FloatTensor(load_images(os.path.join(img_path,sequence,'image_0'),img_size)))\n",
    "        odometry_set.append(torch.FloatTensor(load_poses(os.path.join(poses_path,sequence+'.txt'))))\n",
    "    \n",
    "    return images_set, odometry_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubp\\Downloads\\lectures\\cv\\project\n",
      "images  C:\\Users\\shubp\\Downloads\\lectures\\cv\\project\\data_odometry_gray\\dataset\\sequences\\03\\image_0\n",
      "images count :  800\n",
      "pose  C:\\Users\\shubp\\Downloads\\lectures\\cv\\project\\data_odometry_gray\\dataset\\poses\\03.txt\n",
      "poses count:  800\n"
     ]
    }
   ],
   "source": [
    "#dataload\n",
    "X,y = VODataLoader(\"C:\\\\Users\\\\shubp\\\\Downloads\\\\lectures\\\\cv\\\\project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([800, 6, 384, 1280])\n",
      "Details of y :\n",
      "torch.Size([800, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Details of X :\")\n",
    "# print(type(X)) \n",
    "# print(type(X[0]))\n",
    "# print(len(X)) \n",
    "# print(len(X[0])) \n",
    "print(X[0].size())\n",
    "print(\"Details of y :\")\n",
    "# print(type(y))\n",
    "# print(type(y[0]))\n",
    "# print(len(y))\n",
    "# print(len(y[0]))\n",
    "print(y[0].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Details of X :\n",
      "torch.Size([80, 10, 6, 384, 1280])\n",
      "Details of y :\n",
      "torch.Size([80, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "#Converting lists containing tensors to tensors as per the batchsize (10)\n",
    "X=torch.stack(X).view(-1,10,6, 384, 1280)\n",
    "y=torch.stack(y).view(-1,10,6)\n",
    "print(\"Details of X :\")\n",
    "print(X.size())\n",
    "print(\"Details of y :\")\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to display image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img):\n",
    "    plt.figure\n",
    "    plt.imshow(img, 'gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining neural network as per RP by Sen Wang\n",
    "class DeepVONet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepVONet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)) #6 64\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d (64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d (128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.conv3_1 = nn.Conv2d (256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.conv4 = nn.Conv2d (256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.conv4_1 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.conv5 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.conv5_1 = nn.Conv2d (512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.conv6 = nn.Conv2d (512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.lstm1 = nn.LSTMCell(20*6*1024, 100)\n",
    "        self.lstm2 = nn.LSTMCell(100, 100)\n",
    "        self.fc = nn.Linear(in_features=100, out_features=6)\n",
    "\n",
    "        self.reset_hidden_states()\n",
    "\n",
    "    def reset_hidden_states(self, size=10, zero=True):\n",
    "        if zero == True:\n",
    "            self.hx1 = Variable(torch.zeros(size, 100))\n",
    "            self.cx1 = Variable(torch.zeros(size, 100))\n",
    "            self.hx2 = Variable(torch.zeros(size, 100))\n",
    "            self.cx2 = Variable(torch.zeros(size, 100))\n",
    "        else:\n",
    "            self.hx1 = Variable(self.hx1.data)\n",
    "            self.cx1 = Variable(self.cx1.data)\n",
    "            self.hx2 = Variable(self.hx2.data)\n",
    "            self.cx2 = Variable(self.cx2.data)\n",
    "\n",
    "        if next(self.parameters()).is_cuda == True:\n",
    "            self.hx1 = self.hx1.cuda()\n",
    "            self.cx1 = self.cx1.cuda()\n",
    "            self.hx2 = self.hx2.cuda()\n",
    "            self.cx2 = self.cx2.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.conv3_1(x)\n",
    "        x = self.relu3_1(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.conv4_1(x)\n",
    "        x = self.relu4_1(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.conv5_1(x)\n",
    "        x = self.relu5_1(x)\n",
    "        x = self.conv6(x)\n",
    "        #print(x.size())\n",
    "        x = x.view(x.size(0), 20 * 6 * 1024)\n",
    "        #print(x.size())\n",
    "        self.hx1, self.cx1 = self.lstm1(x, (self.hx1, self.cx1))\n",
    "        x = self.hx1\n",
    "        self.hx2, self.cx2 = self.lstm2(x, (self.hx2, self.cx2))\n",
    "        x = self.hx2\n",
    "        #print(x.size())\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def training_model(model, train_num, X, y, epoch_num=25):\n",
    "    start_time = time.time()\n",
    "    for epoch in range(epoch_num):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        print(\"Epoch : \", epoch+1)\n",
    "        for i in range(train_num):\n",
    "            print(\"Train num :\", i+1)\n",
    "            inputs = X[i]\n",
    "            #print(len(inputs))\n",
    "            labels = y[i]\n",
    "            #print(len(labels))\n",
    "            model.zero_grad()\n",
    "            model.reset_hidden_states()\n",
    "            #optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            if(epoch == (epoch_num-1) and (i > train_num-5)):\n",
    "                print(\"Outputs \", outputs)\n",
    "                print(\"Labels \", labels)\n",
    "            #print(outputs)\n",
    "            #print(labels)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('Epoch : %d Loss: %.3f' %(epoch+1, running_loss/train_num))\n",
    "\n",
    "\n",
    "    print('Finished Training')\n",
    "    print (\"Time taken in Training {0}\".format((time.time() - start_time)))\n",
    "\n",
    "#Testing functions, it is predicting the output for test sequence as per the model\n",
    "def testing_model (model, test_num, X):\n",
    "    start_time = time.time()\n",
    "    Y_output = []\n",
    "    count = 0\n",
    "    totcount = 0\n",
    "    for i in range(test_num):\n",
    "            # get the inputs\n",
    "            inputs = X[i]\n",
    "            outputs = model(inputs)\n",
    "            Y_output.append(outputs)\n",
    "    print (\"Time taken in Testing {0}\".format((time.time() - start_time)))\n",
    "    return torch.stack(Y_output)\n",
    "\n",
    "#Helper functions to get accuracy\n",
    "def get_accuracy(outputs, labels, batch_size):\n",
    "    diff =0\n",
    "    for i in range(batch_size):\n",
    "        for j in range(10):\n",
    "            out = outputs[j].numpy()\n",
    "            lab = labels[j].numpy()\n",
    "            diff+=get_mse_diff(out,lab)\n",
    "    #print(\"Loss : \",diff/(batch_size*10),\"%\")\n",
    "    print(\"Accuracy : \",(1 -diff/(batch_size*10))*100,\"%\")\n",
    "    \n",
    "def get_mse_diff(x,y):\n",
    "    diff= 0\n",
    "    for i in range(6):\n",
    "        diff += (x[i]-y[i])*(x[i]-y[i])\n",
    "    return diff/6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepVONet(\n",
      "  (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
      "  (relu1): ReLU(inplace)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu2): ReLU(inplace)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
      "  (relu3): ReLU(inplace)\n",
      "  (conv3_1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu3_1): ReLU(inplace)\n",
      "  (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu4): ReLU(inplace)\n",
      "  (conv4_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu4_1): ReLU(inplace)\n",
      "  (conv5): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (relu5): ReLU(inplace)\n",
      "  (conv5_1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (relu5_1): ReLU(inplace)\n",
      "  (conv6): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "  (lstm1): LSTMCell(122880, 100)\n",
      "  (lstm2): LSTMCell(100, 100)\n",
      "  (fc): Linear(in_features=100, out_features=6, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#Creating model and defining loss and optimizer to be used \n",
    "model = DeepVONet()\n",
    "print(model)\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5, weight_decay=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment lines below to see model paramters \n",
    "# for parameter in model.parameters():\n",
    "#     print(len(parameter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1\n",
      "Train num : 1\n",
      "Train num : 2\n",
      "Train num : 3\n",
      "Train num : 4\n",
      "Train num : 5\n",
      "Train num : 6\n",
      "Train num : 7\n",
      "Train num : 8\n",
      "Train num : 9\n",
      "Train num : 10\n",
      "Epoch : 1 Loss: 0.072\n",
      "Epoch :  2\n",
      "Train num : 1\n",
      "Train num : 2\n",
      "Train num : 3\n",
      "Train num : 4\n",
      "Train num : 5\n",
      "Train num : 6\n",
      "Train num : 7\n",
      "Outputs  tensor([[ 0.0703,  0.0836,  0.0183, -0.0317, -0.0713, -0.0372],\n",
      "        [ 0.0738,  0.0853,  0.0192, -0.0330, -0.0737, -0.0390],\n",
      "        [ 0.0654,  0.0889,  0.0242, -0.0311, -0.0752, -0.0394],\n",
      "        [ 0.0694,  0.0866,  0.0254, -0.0235, -0.0697, -0.0328],\n",
      "        [ 0.0681,  0.0851,  0.0265, -0.0274, -0.0709, -0.0324],\n",
      "        [ 0.0693,  0.0817,  0.0260, -0.0263, -0.0725, -0.0353],\n",
      "        [ 0.0673,  0.0809,  0.0290, -0.0232, -0.0708, -0.0374],\n",
      "        [ 0.0680,  0.0792,  0.0293, -0.0332, -0.0757, -0.0427],\n",
      "        [ 0.0679,  0.0776,  0.0274, -0.0288, -0.0729, -0.0412],\n",
      "        [ 0.0694,  0.0771,  0.0284, -0.0258, -0.0668, -0.0341]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "Labels  tensor([[ 0.0206, -0.0015,  0.3970, -0.0017,  0.0077,  0.0004],\n",
      "        [ 0.0210,  0.0004,  0.3719, -0.0014,  0.0080, -0.0019],\n",
      "        [ 0.0204,  0.0004,  0.3628, -0.0006,  0.0091, -0.0036],\n",
      "        [ 0.0183, -0.0017,  0.3111, -0.0003,  0.0084, -0.0048],\n",
      "        [ 0.0234, -0.0040,  0.2982, -0.0010,  0.0090, -0.0038],\n",
      "        [ 0.0299, -0.0089,  0.2742, -0.0009,  0.0093,  0.0004],\n",
      "        [ 0.0332, -0.0075,  0.2657,  0.0014,  0.0109,  0.0011],\n",
      "        [ 0.0361, -0.0068,  0.2590,  0.0041,  0.0123,  0.0016],\n",
      "        [ 0.0392, -0.0059,  0.2568,  0.0034,  0.0135, -0.0006],\n",
      "        [ 0.0410, -0.0071,  0.2547,  0.0007,  0.0158, -0.0018]])\n",
      "Train num : 8\n",
      "Outputs  tensor([[ 0.0716,  0.0839,  0.0304, -0.0288, -0.0630, -0.0314],\n",
      "        [ 0.0707,  0.0813,  0.0284, -0.0239, -0.0674, -0.0343],\n",
      "        [ 0.0754,  0.0760,  0.0288, -0.0310, -0.0713, -0.0350],\n",
      "        [ 0.0728,  0.0745,  0.0252, -0.0246, -0.0674, -0.0335],\n",
      "        [ 0.0737,  0.0739,  0.0221, -0.0206, -0.0664, -0.0298],\n",
      "        [ 0.0722,  0.0732,  0.0224, -0.0250, -0.0703, -0.0323],\n",
      "        [ 0.0719,  0.0763,  0.0210, -0.0260, -0.0725, -0.0315],\n",
      "        [ 0.0704,  0.0836,  0.0235, -0.0230, -0.0689, -0.0327],\n",
      "        [ 0.0686,  0.0846,  0.0211, -0.0265, -0.0689, -0.0292],\n",
      "        [ 0.0726,  0.0836,  0.0166, -0.0285, -0.0697, -0.0334]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "Labels  tensor([[ 0.0483, -0.0055,  0.2626, -0.0001,  0.0172, -0.0020],\n",
      "        [ 0.0507, -0.0055,  0.2365,  0.0001,  0.0165, -0.0004],\n",
      "        [ 0.0606, -0.0052,  0.2444,  0.0001,  0.0185,  0.0007],\n",
      "        [ 0.0675, -0.0055,  0.2467, -0.0007,  0.0204,  0.0007],\n",
      "        [ 0.0670, -0.0045,  0.2299, -0.0001,  0.0201, -0.0006],\n",
      "        [ 0.0737, -0.0049,  0.2380,  0.0005,  0.0220, -0.0020],\n",
      "        [ 0.0814, -0.0062,  0.2362,  0.0003,  0.0236, -0.0013],\n",
      "        [ 0.0903, -0.0076,  0.2387, -0.0011,  0.0256, -0.0017],\n",
      "        [ 0.0960, -0.0086,  0.2391, -0.0021,  0.0271, -0.0037],\n",
      "        [ 0.1065, -0.0099,  0.2414, -0.0020,  0.0296, -0.0033]])\n",
      "Train num : 9\n",
      "Outputs  tensor([[ 0.0695,  0.0818,  0.0223, -0.0314, -0.0742, -0.0311],\n",
      "        [ 0.0643,  0.0831,  0.0260, -0.0202, -0.0707, -0.0386],\n",
      "        [ 0.0675,  0.0885,  0.0239, -0.0267, -0.0685, -0.0363],\n",
      "        [ 0.0746,  0.0828,  0.0244, -0.0260, -0.0687, -0.0341],\n",
      "        [ 0.0612,  0.0861,  0.0228, -0.0290, -0.0719, -0.0360],\n",
      "        [ 0.0657,  0.0825,  0.0258, -0.0331, -0.0695, -0.0406],\n",
      "        [ 0.0676,  0.0755,  0.0247, -0.0267, -0.0617, -0.0357],\n",
      "        [ 0.0700,  0.0850,  0.0284, -0.0238, -0.0673, -0.0344],\n",
      "        [ 0.0707,  0.0802,  0.0264, -0.0277, -0.0707, -0.0329],\n",
      "        [ 0.0676,  0.0799,  0.0255, -0.0305, -0.0724, -0.0324]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "Labels  tensor([[ 0.1150, -0.0105,  0.2291, -0.0014,  0.0303, -0.0014],\n",
      "        [ 0.1307, -0.0098,  0.2261,  0.0005,  0.0326,  0.0018],\n",
      "        [ 0.1451, -0.0091,  0.2245,  0.0018,  0.0362,  0.0031],\n",
      "        [ 0.1519, -0.0090,  0.2199,  0.0026,  0.0379,  0.0012],\n",
      "        [ 0.1628, -0.0095,  0.2187,  0.0023,  0.0397, -0.0009],\n",
      "        [ 0.1793, -0.0107,  0.2251,  0.0010,  0.0418, -0.0018],\n",
      "        [ 0.1951, -0.0122,  0.2287, -0.0011,  0.0439, -0.0039],\n",
      "        [ 0.2071, -0.0155,  0.2220, -0.0040,  0.0449, -0.0067],\n",
      "        [ 0.2294, -0.0165,  0.2320, -0.0054,  0.0466, -0.0082],\n",
      "        [ 0.2566, -0.0180,  0.2266, -0.0030,  0.0492, -0.0036]])\n",
      "Train num : 10\n",
      "Outputs  tensor([[ 0.0708,  0.0764,  0.0254, -0.0233, -0.0619, -0.0362],\n",
      "        [ 0.0633,  0.0809,  0.0283, -0.0283, -0.0646, -0.0336],\n",
      "        [ 0.0675,  0.0838,  0.0248, -0.0258, -0.0665, -0.0314],\n",
      "        [ 0.0666,  0.0823,  0.0274, -0.0245, -0.0748, -0.0336],\n",
      "        [ 0.0727,  0.0848,  0.0301, -0.0234, -0.0675, -0.0387],\n",
      "        [ 0.0650,  0.0857,  0.0331, -0.0213, -0.0707, -0.0364],\n",
      "        [ 0.0721,  0.0841,  0.0286, -0.0274, -0.0745, -0.0434],\n",
      "        [ 0.0737,  0.0781,  0.0298, -0.0226, -0.0697, -0.0392],\n",
      "        [ 0.0692,  0.0823,  0.0299, -0.0314, -0.0712, -0.0402],\n",
      "        [ 0.0637,  0.0802,  0.0286, -0.0294, -0.0705, -0.0406]],\n",
      "       grad_fn=<ThAddmmBackward>)\n",
      "Labels  tensor([[ 0.2878, -0.0129,  0.2155, -0.0032,  0.0516, -0.0006],\n",
      "        [ 0.3236, -0.0001,  0.2162, -0.0026,  0.0525, -0.0013],\n",
      "        [ 0.3430, -0.0055,  0.2047, -0.0042,  0.0551, -0.0038],\n",
      "        [ 0.3611,  0.0039,  0.1939, -0.0014,  0.0559, -0.0019],\n",
      "        [ 0.3855, -0.0068,  0.1857, -0.0059,  0.0568, -0.0048],\n",
      "        [ 0.4069, -0.0053,  0.1658, -0.0098,  0.0556, -0.0075],\n",
      "        [ 0.4322, -0.0252,  0.1486, -0.0167,  0.0552, -0.0144],\n",
      "        [ 0.4431, -0.0230,  0.1345, -0.0271,  0.0530, -0.0263],\n",
      "        [ 0.4623, -0.0250,  0.1030, -0.0144,  0.0523, -0.0160],\n",
      "        [ 0.4818, -0.0312,  0.0808,  0.0062,  0.0500,  0.0088]])\n",
      "Epoch : 2 Loss: 0.068\n",
      "Finished Training\n",
      "Time taken in Training 513.1482927799225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_model(model,10,X,y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model\n",
    "#torch.save(model.state_dict(), 'DeepVO.pt')\n",
    "#Load model\n",
    "# model_loaded = torch.load('DeepVO.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shubp\\Downloads\\lectures\\cv\\project\n",
      "images  C:\\Users\\shubp\\Downloads\\lectures\\cv\\project\\data_odometry_gray\\dataset\\sequences\\01\\image_0\n",
      "images count :  1101\n",
      "pose  C:\\Users\\shubp\\Downloads\\lectures\\cv\\project\\data_odometry_gray\\dataset\\poses\\01.txt\n",
      "torch.Size([110, 10, 6, 384, 1280])\n",
      "torch.Size([110, 10, 6])\n"
     ]
    }
   ],
   "source": [
    "X_test,y_test = VODataLoader(\"C:\\\\Users\\\\shubp\\\\Downloads\\\\lectures\\\\cv\\\\project\", test=True)\n",
    "X_test=torch.stack(X_test).view(-1,10,6, 384, 1280)\n",
    "y_test=torch.stack(y_test).view(-1,10,6)\n",
    "print(X_test.size())\n",
    "print(y_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  97.41385879973072 %\n"
     ]
    }
   ],
   "source": [
    "#Getting predictions from the model \n",
    "test_batch_size = 2 #Based on this count only batches will be process, not as per the total number of batches provided\n",
    "y_output = testing_model(model,test_batch_size,X)\n",
    "print(Y_output.size())\n",
    "#Saving outputs\n",
    "torch.save(y_output,\"y_output.pt\")\n",
    "#getting accuracy\n",
    "get_accuracy(y_output,y_test,test_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
